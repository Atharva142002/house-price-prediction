# -*- coding: utf-8 -*-
"""house price prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Htau0Ht4AJWtazw4HNIqYQyJRI228mlw
"""

pip install jupyterlab

import numpy as np
import pandas as pd
import tensorflow as tf
import math
import shutil
from IPython.core import display as ICD

df = pd.read_csv("/content/housing[1].csv", sep=",")
print('Original Dataset:')
ICD.display(df.head(15))
a = pd.DataFrame(df.isnull().sum())
a['# of null values'] = a[0]
b = a[['# of null values']]
print('Before Dropping Null Values:')
print('# of Rows, Columns: ',df.shape)
ICD.display(b)
df = df.dropna(axis=0)
a = pd.DataFrame(df.isnull().sum())
a['# of null values'] = a[0]
b = a[['# of null values']]
print('After Dropping Null Values:')
print('# of Rows, Columns: ',df.shape)
ICD.display(b)

# Handle null values
df = df.dropna(axis=0)

# Create new features
df['num_rooms'] = df['housing_median_age'] / df['households']
df['num_bedrooms'] = df['median_income'] / df['households']
df['persons_per_house'] = df['population'] / df['households']

a = pd.DataFrame(df.isnull().sum())
a['# of null values'] = a[0]
b = a[['# of null values']]
print('Before Dropping Null Values:')
print('# of Rows, Columns: ',df.shape)
ICD.display(b)
df = df.dropna(axis=0)
a = pd.DataFrame(df.isnull().sum())
a['# of null values'] = a[0]
b = a[['# of null values']]
print('After Dropping Null Values:')
print('# of Rows, Columns: ',df.shape)
ICD.display(b)

import pandas as pd
import matplotlib.pyplot as plt

# Generate the scatter matrix plot
scatter_matrix = pd.plotting.scatter_matrix(df, alpha=0.2, figsize=(17, 17), diagonal='hist')

# Show the plot
plt.show()

df.isnull().sum()

df.info()

df['ocean_proximity'].value_counts()

mapping = {'<1H OCEAN': 1,'INLAND': 2,'NEAR OCEAN': 3,'NEAR BAY':4,'ISLAND':5}
df['ocean_proximity'] = df['ocean_proximity'].map(mapping)

df.hist(figsize=(15, 15))

X = df.drop(['median_house_value'],axis=1)
y = df['median_house_value']
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)

reg = LinearRegression()
reg.fit(X_train,y_train)

if 'population' in df.columns:
    # Create the 'households' column
    df['households'] = df['population'] / df['persons_per_house']
else:
    print("The 'population' column does not exist in the DataFrame.")

df['num_rooms'] = df['total_rooms'] / df['households']
df['num_bedrooms'] = df['total_bedrooms'] / df['households']
df['persons_per_house'] = df['population'] / df['households']
df.drop(['total_rooms', 'total_bedrooms', 'population', 'households'], axis = 1, inplace = True)

# Drop the columns using a list comprehension
df = df[[column for column in df.columns if column not in ['total_rooms', 'total_bedrooms', 'population', 'households']]]

# Define feature columns
feature_columns = {}
for colname in ['housing_median_age', 'median_income', 'num_rooms', 'num_bedrooms', 'persons_per_house']:
    feature_columns[colname] = tf.feature_column.numeric_column(colname)

# Bucketize latitude and longitude
longitude_boundaries = np.linspace(-124.3, -114.3, 5).tolist()
latitude_boundaries = np.linspace(32.5, 42, 10).tolist()

feature_columns['longitude'] = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column('longitude'), boundaries=longitude_boundaries)

feature_columns['latitude'] = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column('latitude'), boundaries=latitude_boundaries
)

# Split the data into training and evaluation sets
msk = np.random.rand(len(df)) < 0.8
traindf = df[msk]
evaldf = df[~msk]

!pip install tensorflow-estimator

import tensorflow_estimator.python.estimator.api._v2.estimator.inputs

try:
    import tensorflow_estimator.python.estimator.api._v2.estimator.inputs
except ImportError:
    print("The tensorflow_estimator.python.estimator.api._v2.estimator.inputs module is not installed.")

!pip install tensorflow-estimator

# Ensure the DataFrame has the required columns
required_columns = ['total_rooms', 'total_bedrooms', 'population', 'households']
missing_columns = [col for col in required_columns if col not in df.columns]

if missing_columns:
    print(f"Missing columns: {missing_columns}")
else:
    # Create new columns
    df['num_rooms'] = df['total_rooms'] / df['households']
    df['num_bedrooms'] = df['total_bedrooms'] / df['households']
    df['persons_per_house'] = df['population'] / df['households']

# Define feature columns
feature_columns = {}
for colname in ['housing_median_age', 'median_income', 'num_rooms', 'num_bedrooms', 'persons_per_house']:
    feature_columns[colname] = tf.feature_column.numeric_column(colname)

# Bucketize latitude and longitude
longitude_boundaries = np.linspace(-124.3, -114.3, 5).tolist()
latitude_boundaries = np.linspace(32.5, 42, 10).tolist()

feature_columns['longitude'] = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column('longitude'), boundaries=longitude_boundaries
)
feature_columns['latitude'] = tf.feature_column.bucketized_column(
    tf.feature_column.numeric_column('latitude'), boundaries=latitude_boundaries
)

# Split the data into training and evaluation sets
msk = np.random.rand(len(df)) < 0.8
traindf = df[msk]
evaldf = df[~msk]

# Define input functions using tf.data.Dataset
SCALE = 100000
BATCH_SIZE = 100

def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):
    def input_function():
        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))
        if shuffle:
            ds = ds.shuffle(buffer_size=len(data_df))
        ds = ds.batch(batch_size).repeat(num_epochs)
        return ds
    return input_function

train_input_fn = make_input_fn(
    traindf[list(feature_columns.keys())],
    traindf["median_house_value"] / SCALE,
    num_epochs=1,
    shuffle=True,
    batch_size=BATCH_SIZE
)

eval_input_fn = make_input_fn(
    evaldf[list(feature_columns.keys())],
    evaldf["median_house_value"] / SCALE,
    num_epochs=1,
    shuffle=False,
    batch_size=len(evaldf)
)

# Display the shape of the DataFrame and the first 15 rows
print('# of Rows, Columns:', df.shape)
print(df.head(15))

c = pd.plotting.scatter_matrix(df, alpha=0.2, figsize=(17, 17), diagonal='hist')
c;

def print_rmse(model, name, input_fn):
  metrics = model.evaluate(input_fn=input_fn, steps=1)
  print ('RMSE on {} dataset = {} USD'.format(name, np.sqrt(metrics['average_loss'])*SCALE))

# Define constants
SCALE = 100000

# Define input function
def get_train_input_fn(df):
    dataset = tf.data.Dataset.from_tensor_slices((dict(num_rooms=df[["num_rooms"]]), df["median_house_value"] / SCALE))
    return dataset.shuffle(buffer_size=1000).batch(32)

# Define feature columns
features = [tf.feature_column.numeric_column('num_rooms')]

# Define output directory and optimizer
outdir = './housing_trained'
myopt = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)

# Create and train the model
def train_model(df):
    # Remove previous output directory
    shutil.rmtree(outdir, ignore_errors=True)

    # Create the model
    model = tf.estimator.LinearRegressor(
        model_dir=outdir,
        feature_columns=features,
        optimizer=myopt
    )

    # Train the model
    model.train(input_fn=lambda: get_train_input_fn(df), steps=300)

    # Print RMSE
    print_rmse(model, 'training', lambda: get_train_input_fn(df))

# Call the training function
train_model(df)

import tensorflow as tf
import pandas as pd

# Define constants
SCALE = 100000

# Define input function
def get_train_input_fn(df):
    dataset = tf.data.Dataset.from_tensor_slices((dict(num_rooms=df[["num_rooms"]]), df["median_house_value"] / SCALE))
    return dataset.shuffle(buffer_size=1000).batch(32)

# Define feature columns
features = [tf.feature_column.numeric_column('num_rooms')]

# Define output directory and optimizer
outdir = './housing_trained'
myopt = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)

# Create and train the model
def train_model(df):
    # Remove previous output directory
    shutil.rmtree(outdir, ignore_errors=True)

    # Create the model
    model = tf.estimator.LinearRegressor(
        model_dir=outdir,
        feature_columns=features,
        optimizer=myopt
    )

    # Train the model
    model.train(input_fn=lambda: get_train_input_fn(df), steps=300)

    # Print RMSE
    print_rmse(model, 'training', lambda: get_train_input_fn(df))

# Call the training function
train_model(df)

from tensorflow.keras.optimizers import Adam

model = tf.estimator.LinearRegressor(
    model_dir=outdir,
    feature_columns=features,
    optimizer=Adam(learning_rate=0.01)
)

import tensorflow as tf
import pandas as pd

# Define constants
SCALE = 100000

# Define input function
def get_train_input_fn(df):
    dataset = tf.data.Dataset.from_tensor_slices((dict(num_rooms=df[["num_rooms"]]), df["median_house_value"] / SCALE))
    return dataset.shuffle(buffer_size=1000).batch(32)

# Define feature columns
features = [tf.feature_column.numeric_column('num_rooms')]

# Define output directory and optimizer
outdir = './housing_trained'
optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)

# Create and train the model
def train_model(df):
    # Remove previous output directory
    shutil.rmtree(outdir, ignore_errors=True)

    # Create the model
    model = tf.estimator.LinearRegressor(
        model_dir=outdir,
        feature_columns=features,
        optimizer=optimizer
    )

    # Train the model
    model.train(input_fn=lambda: get_train_input_fn(df), steps=300)

    # Print RMSE
    print_rmse(model, 'training', lambda: get_train_input_fn(df))

# Call the training function
train_model(df)

import tensorflow as tf

# Define output directory and optimizer
outdir = './housing_trained'
optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)

# Create and train the model
def train_model(df):
    # Remove previous output directory
    shutil.rmtree(outdir, ignore_errors=True)

    # Create the model
    model = tf.estimator.LinearRegressor(
        model_dir=outdir,
        feature_columns=features,
        optimizer=optimizer
    )

    # Train the model
    model.train(input_fn=lambda: get_train_input_fn(df), steps=300)

    # Print RMSE
    print_rmse(model, 'training', lambda: get_train_input_fn(df))

# Call the training function
train_model(df)